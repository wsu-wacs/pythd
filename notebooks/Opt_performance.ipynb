{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import sys, time\n",
    "sys.path.append(\"..\")\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import igraph\n",
    "\n",
    "import umap\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pythd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_thd(nrows, ncols, ncover, filt, metric='euclidean', precompute=False):\n",
    "    total_time = time.perf_counter()\n",
    "    setup_time = total_time\n",
    "    \n",
    "    dataset = np.random.normal(size=(nrows, ncols))\n",
    "    \n",
    "    clustering = pythd.clustering.HierarchicalClustering(method='complete', \n",
    "                                                         metric=('precomputed' if precompute else metric))\n",
    "    \n",
    "    filt.reset()\n",
    "    f_x = filt(dataset)\n",
    "    \n",
    "    cov = pythd.cover.IntervalCover.EvenlySpacedFromValues(f_x, ncover, 0.6)\n",
    "\n",
    "    thd = pythd.thd.THD(dataset, filt, cov, clustering=clustering, \n",
    "                    group_threshold=2, contract_amount=0.1, \n",
    "                    precompute=precompute, metric=metric)\n",
    "    \n",
    "    thd_time = time.perf_counter()\n",
    "    setup_time = thd_time - setup_time\n",
    "    \n",
    "    thd.run()\n",
    "    \n",
    "    last_time = time.perf_counter()\n",
    "    total_time = last_time - total_time\n",
    "    thd_time = last_time - thd_time\n",
    "    \n",
    "    return (setup_time, thd_time, total_time)\n",
    "\n",
    "def time_thds(niter, nrows, ncols, ncover, filt, metric='euclidean', precompute=False):\n",
    "    total_times = []\n",
    "    setup_times = []\n",
    "    thd_times = []\n",
    "    \n",
    "    for i in range(niter):\n",
    "        setup_time, thd_time, total_time = time_thd(nrows, ncols, ncover, filt, metric, precompute)\n",
    "        total_times.append(total_time)\n",
    "        setup_times.append(setup_time)\n",
    "        thd_times.append(thd_time)\n",
    "    \n",
    "    return total_times, setup_times, thd_times\n",
    "    \n",
    "def confidence_interval(data, confidence=0.99):\n",
    "    \"\"\"Compute a confidence interval for a series of data.\n",
    "    \n",
    "    Returns half the width of the interval\"\"\"\n",
    "    data = np.array(data)\n",
    "    n = data.shape[0]\n",
    "    sem = sp.stats.sem(data) # standard error of the mean\n",
    "    return sem * sp.stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
    "\n",
    "def format_times(tms):\n",
    "    \"\"\"Format a list of times using a 99% confidence interval\"\"\"\n",
    "    tms = np.array(tms)\n",
    "    mean = tms.mean()\n",
    "    h = confidence_interval(tms)\n",
    "    return \"{:.4f} +/- {:.4f}\".format(mean, h)\n",
    "\n",
    "def get_functs():\n",
    "    def linear(x, a, b):\n",
    "        return x*a + b\n",
    "    def quadrat(x, a, b, c):\n",
    "        return x*(a*x + b) + c\n",
    "    def cubic(x, a, b, c, d):\n",
    "        return x*(x*(a*x + b)+c)+d\n",
    "    def logarithmic(x, a, b, c):\n",
    "        return a*np.log(b*x)+c\n",
    "    def nlogn(x, a, b, c):\n",
    "        return a*x*np.log(b*x)+c\n",
    "    def exponential(x, a, b, c):\n",
    "        return a*np.exp(b*x)+c\n",
    "    def logistic(x, L, k, x0):\n",
    "        return L / (1 + np.exp(-k*(x - x0)))\n",
    "    \n",
    "    # dictionary of functions with parameter bounds\n",
    "    d = {\n",
    "        'linear': (linear, \n",
    "                   (np.array([-np.inf]*2), np.array([np.inf]*2))),\n",
    "        'quadratic': (quadrat,\n",
    "                      (np.array([-np.inf]*3), np.array([np.inf]*3))),\n",
    "        'cubic': (cubic,\n",
    "                  (np.array([-np.inf]*4), np.array([np.inf]*4))),\n",
    "        'logarithmic': (logarithmic,\n",
    "                        (np.array([-np.inf, 0., -np.inf]), np.array([np.inf]*3))),\n",
    "        'nlogn': (nlogn,\n",
    "                  (np.array([-np.inf, 0., -np.inf]), np.array([np.inf]*3))),\n",
    "        'exponential': (exponential,\n",
    "                        (np.array([-np.inf]*3), np.array([np.inf, 10., np.inf]))),\n",
    "        'logistic': (logistic,\n",
    "                     (np.array([0., 0., -np.inf]), np.array([np.inf, np.inf, np.inf])))\n",
    "    }\n",
    "    return d\n",
    "    \n",
    "def fit_curves(x, y):\n",
    "    d = get_functs()\n",
    "    res = []\n",
    "    \n",
    "    for k, tup in d.items():\n",
    "        f, bounds = tup\n",
    "        try:\n",
    "            popt, pcov = curve_fit(f, x, y, bounds=bounds, maxfev=2000)\n",
    "            ssq = np.sum((y - f(x, *popt))**2)\n",
    "            res.append((k, popt, ssq))\n",
    "        except (RuntimeError, ValueError) as e:\n",
    "            print(k, e)\n",
    "    \n",
    "    res.sort(key=lambda zz: zz[2], reverse=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-bloom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# varying dataset size\n",
    "DATASET_SIZES = np.logspace(1, 3.8, num=60, base=10.0, dtype=int)\n",
    "orig_means = []\n",
    "orig_intervals = []\n",
    "opt_means = []\n",
    "opt_intervals = []\n",
    "\n",
    "for sz in DATASET_SIZES:\n",
    "    print(\"Dataset size\", sz)\n",
    "    filt = pythd.filter.ScikitLearnFilter(PCA, n_components=2)\n",
    "    \n",
    "    orig_times = time_thds(50, nrows=sz, ncols=2, ncover=10, filt=filt, precompute=False)[0]\n",
    "    orig_means.append(np.mean(orig_times))\n",
    "    orig_intervals.append(confidence_interval(orig_times, confidence=0.95))\n",
    "    \n",
    "    opt_times = time_thds(50, nrows=sz, ncols=2, ncover=10, filt=filt, precompute=True)[0]\n",
    "    opt_means.append(np.mean(orig_means))\n",
    "    opt_intervals.append(confidence_interval(opt_times, confidence=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot varying dataset size\n",
    "orig_means = np.array(orig_means)\n",
    "orig_intervals = np.array(orig_intervals)\n",
    "plt.plot(DATASET_SIZES, orig_means, \"-\", color='blue', label='unoptimized')\n",
    "plt.fill_between(DATASET_SIZES, orig_means - orig_intervals, orig_means + orig_intervals, alpha=0.3, color='blue')\n",
    "\n",
    "opt_means = np.array(opt_means)\n",
    "opt_intervals = np.array(opt_intervals)\n",
    "plt.plot(DATASET_SIZES, opt_means, '-', color='red', label='optimized')\n",
    "plt.fill_between(DATASET_SIZES, opt_means - opt_intervals, opt_means + opt_intervals, alpha=0.3, color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('dataset size (rows)')\n",
    "plt.ylabel('average runtime (s)')\n",
    "plt.show()\n",
    "\n",
    "# Logarithmic for smaller sizes, linear for larger? What is the turning point?\n",
    "min_ssq = np.inf\n",
    "functs = get_functs()\n",
    "for thresh in DATASET_SIZES[5:-5]:\n",
    "    below = DATASET_SIZES < thresh\n",
    "    log_f, log_bound = functs['logarithmic']\n",
    "    log_parms, _ = curve_fit(log_f, DATASET_SIZES[below], opt_means[below], bounds=log_bound, maxfev=2000)\n",
    "    log_ssq = np.sum((opt_means[below] - log_f(DATASET_SIZES[below], *log_parms)) ** 2)\n",
    "\n",
    "    lin_f, lin_bound = functs['linear']\n",
    "    lin_parms, _ = curve_fit(lin_f, DATASET_SIZES[~below], opt_means[~below], bounds=lin_bound, maxfev=2000)\n",
    "    lin_ssq = np.sum((opt_means[~below]))\n",
    "    # Smallest sum-of-squares error so far\n",
    "    if (log_ssq + lin_ssq) < min_ssq:\n",
    "        min_ssq = log_ssq + lin_ssq\n",
    "        best_thresh = thresh\n",
    "        best_log_parms = log_parms\n",
    "        best_lin_parms = lin_parms\n",
    "\n",
    "print(best_thresh, min_ssq)\n",
    "below = DATASET_SIZES < best_thresh\n",
    "\n",
    "plt.plot(DATASET_SIZES, opt_means, '--', color='red', label='actual')\n",
    "plt.plot(DATASET_SIZES[below], log_f(DATASET_SIZES[below], *best_log_parms), label='logarithmic')\n",
    "plt.plot(DATASET_SIZES[~below], lin_f(DATASET_SIZES[~below], *best_lin_parms), label='linear')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('dataset size (rows)')\n",
    "plt.ylabel('average runtime (s)')\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-papua",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# varying dataset dimension\n",
    "DATASET_DIMS = np.array(range(10, 61))\n",
    "orig_means = []\n",
    "orig_intervals = []\n",
    "opt_means = []\n",
    "opt_intervals = []\n",
    "\n",
    "for dim in DATASET_DIMS:\n",
    "    print(\"Dataset dimension\", dim)\n",
    "    filt = pythd.filter.ScikitLearnFilter(PCA, n_components=2)\n",
    "    \n",
    "    orig_times = time_thds(50, nrows=1000, ncols=dim, ncover=10, filt=filt, precompute=False)[0]\n",
    "    orig_means.append(np.mean(orig_times))\n",
    "    orig_intervals.append(confidence_interval(orig_times, confidence=0.95))\n",
    "    \n",
    "    opt_times = time_thds(50, nrows=1000, ncols=dim, ncover=10, filt=filt, precompute=True)[0]\n",
    "    opt_means.append(np.mean(orig_means))\n",
    "    opt_intervals.append(confidence_interval(opt_times, confidence=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot varying dataset dimension\n",
    "orig_means = np.array(orig_means)\n",
    "orig_intervals = np.array(orig_intervals)\n",
    "plt.plot(DATASET_DIMS, orig_means, \"-\", color='blue', label='unoptimized')\n",
    "plt.fill_between(DATASET_DIMS, orig_means - orig_intervals, orig_means + orig_intervals, alpha=0.3, color='blue')\n",
    "\n",
    "opt_means = np.array(opt_means)\n",
    "opt_intervals = np.array(opt_intervals)\n",
    "plt.plot(DATASET_DIMS, opt_means, '-', color='red', label='optimized')\n",
    "plt.fill_between(DATASET_DIMS, opt_means - opt_intervals, opt_means + opt_intervals, alpha=0.3, color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('dataset dimension (columns)')\n",
    "plt.ylabel('average runtime (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-sunglasses",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# varying number of open sets in the cover\n",
    "COVER_COUNTS = np.array(range(2, 51))\n",
    "orig_means = []\n",
    "orig_intervals = []\n",
    "opt_means = []\n",
    "opt_intervals = []\n",
    "\n",
    "for ncov in COVER_COUNTS:\n",
    "    print(ncov, \"covers\")\n",
    "    filt = pythd.filter.ScikitLearnFilter(PCA, n_components=2)\n",
    "    \n",
    "    orig_times = time_thds(50, nrows=200, ncols=2, ncover=int(ncov), filt=filt, precompute=False)[0]\n",
    "    orig_means.append(np.mean(orig_times))\n",
    "    orig_intervals.append(confidence_interval(orig_times, confidence=0.95))\n",
    "    \n",
    "    opt_times = time_thds(50, nrows=200, ncols=2, ncover=int(ncov), filt=filt, precompute=True)[0]\n",
    "    opt_means.append(np.mean(orig_means))\n",
    "    opt_intervals.append(confidence_interval(opt_times, confidence=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot varying open set numbers\n",
    "orig_means = np.array(orig_means)\n",
    "orig_intervals = np.array(orig_intervals)\n",
    "plt.plot(COVER_COUNTS, orig_means, \"-\", color='blue', label='unoptimized')\n",
    "plt.fill_between(COVER_COUNTS, orig_means - orig_intervals, orig_means + orig_intervals, alpha=0.3, color='blue')\n",
    "\n",
    "opt_means = np.array(opt_means)\n",
    "opt_intervals = np.array(opt_intervals)\n",
    "plt.plot(COVER_COUNTS, opt_means, '-', color='red', label='optimized')\n",
    "plt.fill_between(COVER_COUNTS, opt_means - opt_intervals, opt_means + opt_intervals, alpha=0.3, color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('number of open sets in cover')\n",
    "plt.ylabel('average runtime (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-rolling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
