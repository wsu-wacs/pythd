{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This THD script uses mapper written by XH (from scratch). \n",
    "A re-written mapper is required in order to avoid stack overflow issues when calculating point-cloud distance in large datasets. \n",
    "\n",
    "Contact: Xiu Huan Yap, yap.4@wright.edu  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import graphviz as gv\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Mapper:\n",
    "    def __init__(self, points, sklearn_filter_function,gain,interval,training_labels=None):\n",
    "        self.points=np.array(points)\n",
    "        self.sklearn_filter_function=sklearn_filter_function \n",
    "        \n",
    "        self.gain=gain\n",
    "        self.interval=interval #Note that interval is the number of filter levels in 1D. i.e. resolution = interval^2\n",
    "        \n",
    "        self.training_labels=np.array(training_labels) #Needed if drawing mapper with training labels\n",
    "        \n",
    "        self.filter_values = self._get_filter_values()\n",
    "        if type(self.training_labels) != None:\n",
    "            self.label_names, self.label_colors=self._get_label_colors()  \n",
    "            \n",
    "    def _get_filter_values(self):\n",
    "        filter_values=Filter(self.points,self.sklearn_filter_function).filter_values\n",
    "        return filter_values\n",
    "\n",
    "    def _get_label_colors(self):\n",
    "        label_names=np.unique(self.training_labels)\n",
    "        golden_ratio_conjugate=0.618033988749895\n",
    "        hue=0\n",
    "        label_colors=[]\n",
    "        for i in range(len(label_names)):\n",
    "            color=str(hue)+\" 0.5\"+ \" 0.95\"\n",
    "            label_colors.append(color)\n",
    "            hue=hue+golden_ratio_conjugate\n",
    "            hue=hue%1\n",
    "        return label_names,label_colors\n",
    "    \n",
    "    def get_mapper(self,store_filter_levels=False):\n",
    "        #Get covering from self.filter_values\n",
    "        cover=Covering(self.filter_values,self.gain,self.interval)\n",
    "        all_bin_edges=cover.all_bin_edges\n",
    "        intervals=cover.intervals\n",
    "        \n",
    "        #Get points into filter levels\n",
    "        point_idx_per_level=self._get_point_idx_per_level(self.filter_values,intervals,all_bin_edges)\n",
    "        nodes={}\n",
    "        node_count=0\n",
    "        node_to_filter_level=[]\n",
    "        for filter_level in point_idx_per_level.keys():\n",
    "            point_idx_in_level=point_idx_per_level[filter_level]\n",
    "            if len(point_idx_in_level)==1:\n",
    "                clusters=[[0]]\n",
    "            elif len(point_idx_in_level)<1:\n",
    "                continue\n",
    "            else:\n",
    "                point_values_in_level=self.points[point_idx_in_level]\n",
    "                clusters=self._mapper_step(point_values_in_level) #List of list of points in same cluster\n",
    "            if len(clusters)>0:\n",
    "                for cluster_idx, cluster in enumerate(clusters):\n",
    "                    node_name=node_count\n",
    "                    node_count+=1\n",
    "                    if (store_filter_levels):\n",
    "                        node_filter_level=filter_level+\".\"+str(cluster_idx)\n",
    "                        node_to_filter_level.append(node_filter_level)\n",
    "                    nodes[node_name]=np.array(point_idx_in_level)[cluster]\n",
    "        edges={}\n",
    "        for pair in itertools.combinations(nodes.keys(),2): #Gets all possible combinations of 2 nodes\n",
    "            common_points=list(set(nodes[pair[0]]).intersection(set(nodes[pair[1]]))) #Gets common points between 2 nodes\n",
    "            if len(common_points)>0:\n",
    "                edges[pair]=common_points\n",
    "        #Save info\n",
    "        self.mapper_result={'nodes':nodes,'edges':edges}\n",
    "        return self.mapper_result\n",
    "    \n",
    "    def draw_mapper(self,model_entry=None,mapper_result=None,points_list=None,label_method='mode'):\n",
    "        '''\n",
    "        Either define model_entry or define mapper_result, points_list\n",
    "        \n",
    "        model_entry: <dict> contains keys mapper_result and points_list\n",
    "        mapper_result: <dict> can be found under key 'mapper_result' for each output in model\n",
    "        points_list: <list> Idx of points from the original training data. Need it to get the labels\n",
    "        label_method: <str> 'mode', 'fraction'\n",
    "                        In 'mode', node is colored by the most popular label\n",
    "                        In 'fraction', node is colored by proportion of labels \n",
    "        '''\n",
    "        if model_entry:\n",
    "            mapper_result=model_entry['mapper_result']\n",
    "            points_list=model_entry['points_list']\n",
    "        elif type(mapper_result) ==None or type(points_list) ==None:\n",
    "            raise ValueError('Parameters mapper_result and points_list not defined')\n",
    "        \n",
    "        h=gv.Digraph(name='mapper_graph',engine='neato',format='png')\n",
    "        h.graph_attr['size']=\"10,10\" #Hardcoded graph size\n",
    "        #.graph_attr['pack']='false'\n",
    "        h.edge_attr['dir']=\"none\"\n",
    "        nodes=mapper_result['nodes']\n",
    "        edges=mapper_result['edges']\n",
    "        for node in nodes:\n",
    "            node_size=float(np.log(len(nodes[node])/200+1)) #Arbitrary hard-coded size constraints\n",
    "            if label_method=='mode':\n",
    "                node_label=mode(self.training_labels[points_list[nodes[node]]])[0][0]\n",
    "                node_color=str(self.label_colors[np.where(self.label_names==node_label)[0][0]])\n",
    "            elif label_method=='fraction':\n",
    "                label_count=[]\n",
    "                for node_label in self.label_names:\n",
    "                    count=np.count_nonzero(self.training_labels[nodes[node]]==node_label)\n",
    "                    label_count.append(count)\n",
    "                label_count=np.divide(label_count,float(sum(label_count)))\n",
    "                node_color=str()\n",
    "                for idx, freq in enumerate(label_count):\n",
    "                    if freq >0:\n",
    "                        node_color=node_color+self.label_colors[idx]+';'+str(freq)+':'\n",
    "            else:\n",
    "                node_color='yellow'\n",
    "            h.node(str(node),shape='circle',label=str(len(nodes[node])),height=str(node_size),style='wedged',fillcolor=node_color)\n",
    "        for edge in edges:\n",
    "            h.edge(str(edge[0]),str(edge[1]),penwidth=str(np.log(len(edges[edge])+1)))\n",
    "        \n",
    "        #Render mapper graph, legend and paste onto combined mapper_image\n",
    "        #mapper_graph=Image.open(h.render())\n",
    "        mapper_image=h\n",
    "        if label_method!=None:\n",
    "            legend=self._get_legend_as_graph()\n",
    "            return mapper_image, legend\n",
    "        #mapper_image_size=(mapper_graph.size[0]+legend.size[0],mapper_graph.size[1])\n",
    "        #mapper_image=Image.new('RGB',mapper_image_size,color='white')\n",
    "        #mapper_image.paste(mapper_graph,(0,0))\n",
    "        #legend_pos=(mapper_graph.size[0],mapper_graph.size[1]-legend.size[1])\n",
    "        #mapper_image.paste(legend,legend_pos)\n",
    "        return mapper_image\n",
    "\n",
    "    def _get_legend_as_graph(self):\n",
    "        h=gv.Digraph(engine='dot',format='png')\n",
    "        h.graph_attr['size']=str(2) #Hardcoded legend size\n",
    "        h.graph_attr['rankdir']='LR'\n",
    "        h.graph_attr['mindist']='0.0'\n",
    "        h.graph_attr['ranksep']='0.0'\n",
    "        h.graph_attr['nodesep']='0.0'\n",
    "        h.node_attr['shape']='box'\n",
    "        h.node_attr['margin']=\"0,0\"\n",
    "        h.node_attr['width']=\"1\"\n",
    "        h.node_attr['height']=\"0.5\"\n",
    "        h.edge_attr['style']='invis'\n",
    "        #h.edge_attr['minlen']='0'\n",
    "        for idx, label in enumerate(self.label_names):\n",
    "            color=str(self.label_colors[idx])\n",
    "            h.node(color[:3],style='filled',fillcolor=color,fontcolor=color)\n",
    "            h.edge('label '+str(label),color[:3])\n",
    "        return h    \n",
    "    \n",
    "    def _mapper_step(self, filter_values_in_level):\n",
    "        #Need to convert filter_value_idx to filter_values_per_level\n",
    "        point_idx_in_clusters=Clustering(filter_values_in_level).point_idx_in_clusters\n",
    "        return point_idx_in_clusters\n",
    "        \n",
    "    \n",
    "    def _get_point_idx_per_level(self, filter_values,intervals,all_bin_edges):\n",
    "    #Returns a dictionary of key:value filter_level:list of indices of points in filter_level\n",
    "    #Only works for 2D\n",
    "        point_idx_per_level={}\n",
    "        for y_interval in range(intervals[1]):\n",
    "            for x_interval in range(intervals[0]):\n",
    "                curr_interval=x_interval+y_interval*intervals[0]\n",
    "                pt_idx_in_filter_level=[]\n",
    "                for idx, filter_value in enumerate(filter_values):\n",
    "                    if (self._check_filter_value(filter_value, all_bin_edges[0][x_interval], all_bin_edges[1][y_interval])):\n",
    "                        pt_idx_in_filter_level.append(idx)\n",
    "                point_idx_per_level[str(curr_interval)]=pt_idx_in_filter_level\n",
    "        return point_idx_per_level\n",
    "\n",
    "    def _check_filter_value(self, filter_value, x_boundaries, y_boundaries):\n",
    "        if filter_value[0]>=x_boundaries[0] and filter_value[0]<=x_boundaries[1]:\n",
    "            if filter_value[1]>=y_boundaries[0] and filter_value[1]<=y_boundaries[1]:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "#These are mapper helper classes\n",
    "\n",
    "class Filter:\n",
    "#For sklearn filter functions\n",
    "    def __init__(self, points, filter_fn):\n",
    "        self.points=points\n",
    "        self.filter_fn=filter_fn\n",
    "        self.filter_values=self._get_filt_values()\n",
    "        \n",
    "    def _get_filt_values(self):\n",
    "        filt=self.filter_fn()\n",
    "        filter_values=filt.fit_transform(self.points)\n",
    "        return filter_values\n",
    "\n",
    "class Covering:\n",
    "#TBD: 2D for now\n",
    "#Adapted from getres (aka Adaptive Resolution) package\n",
    "    def __init__(self, filter_values, gain, interval):\n",
    "        try:\n",
    "            filter_values=filter_values.tolist() #Change np array to python list. Prevent misplacing points in filter levels due to precision. \n",
    "        except:\n",
    "            pass\n",
    "        self.filter_values=filter_values\n",
    "        self.gain=gain\n",
    "        self.interval=interval\n",
    "        \n",
    "        self.dim=len(filter_values[0])\n",
    "        for i in range(1,self.dim):\n",
    "            assert len(filter_values[i-1])==len(filter_values[i]) #Must have same number of points in each dimension\n",
    "        self.filt_min=[min([filter_values[i][j] for i in range(len(filter_values))]) for j in range(self.dim)]\n",
    "        self.filt_max=[max([filter_values[i][j] for i in range(len(filter_values))]) for j in range(self.dim)]\n",
    "        \n",
    "        self.all_bin_edges=self._get_all_bin_edges(self.interval) #3D list\n",
    "        self.intervals=[len(bin_edges) for bin_edges in self.all_bin_edges]\n",
    "        \n",
    "        self.dim=len(self.all_bin_edges)\n",
    "        #self.max_interval_list=[len(all_bin_edges[i]) for i in range(self.dim)] #Gets the number of intervals in each dimension\n",
    "        self.all_spacings,self.all_intervals_in_spacings=self._get_all_spacings()\n",
    "    \n",
    "    def _get_all_bin_edges(self,interval):\n",
    "        all_bin_edges=[]\n",
    "        for i in range(self.dim):\n",
    "            bins=self.bins(interval,self.gain,self.filt_min[i],self.filt_max[i])\n",
    "            bin_edges,overlap_edges=bins.get_bin_edges()\n",
    "            all_bin_edges.append(bin_edges)\n",
    "        return all_bin_edges\n",
    "                \n",
    "    def _get_spacings_in_1D(self,bin_edges_in_1D):\n",
    "        # Divides 1D filter space into (unequal) bins and notes the component intervals that contribute to that bin \n",
    "        # Higher order overlaps will happen only if gain>50%\n",
    "        # Max of res*2 (unequal) bins aka spacings (spacings defined by min value/left edge)\n",
    "        intervals=len(bin_edges_in_1D)\n",
    "        all_edges=sorted(set([i for sublist in bin_edges_in_1D for i in sublist]))[:-1]\n",
    "        spacings=all_edges\n",
    "        intervals_in_spacings=[]\n",
    "        for edge in all_edges:\n",
    "            intervals_in_spacing=[]\n",
    "            for j in range(intervals):\n",
    "                #If min<=edge<max of interval j, then that spacing is contained by interval j\n",
    "                if bin_edges_in_1D[j][0]<=edge and bin_edges_in_1D[j][1]>edge:\n",
    "                    intervals_in_spacing.append(j)\n",
    "            intervals_in_spacings.append(intervals_in_spacing)\n",
    "        return spacings,intervals_in_spacings\n",
    "        \n",
    "    def _get_all_spacings(self):\n",
    "        each_spacings=[]\n",
    "        each_intervals_in_spacings=[]\n",
    "        for i in range(self.dim):\n",
    "            bin_edges_in_1D=self.all_bin_edges[i]\n",
    "            spacings,intervals_in_spacings=self._get_spacings_in_1D(bin_edges_in_1D)\n",
    "            each_spacings.append(spacings)\n",
    "            each_intervals_in_spacings.append(intervals_in_spacings)\n",
    "        all_spacings=[]\n",
    "        all_intervals_in_spacings=[]\n",
    "        #Total number of spacings = multiply no. of spacings in each dimension\n",
    "        #indexes=[0] *self.dim\n",
    "        #max_indexes=[len(all_spacings)[i] for i in range(len(all_spacings))]\n",
    "        #while indexes ~= max_indexes:\n",
    "            \n",
    "        ##Main loop; Gets all spacings and intervals in higher dimension\n",
    "        ##TBD: Hard code to 2D for now.\n",
    "        max_x=self.all_bin_edges[0][-1][-1]\n",
    "        max_y=self.all_bin_edges[1][-1][-1]\n",
    "        if self.dim==2:\n",
    "            x_intervals=self.intervals[0]\n",
    "            y_intervals=self.intervals[1]\n",
    "            x_spacings=len(each_spacings[0])\n",
    "            y_spacings=len(each_spacings[1])\n",
    "            for i2 in range(y_spacings):\n",
    "                for i1 in range(x_spacings):\n",
    "                    #Get bin edges\n",
    "                    if i1==x_spacings-1 and i2<y_spacings-1:\n",
    "                        min_edge=[each_spacings[0][i1],each_spacings[1][i2]]\n",
    "                        max_edge=[max_x,each_spacings[1][i2+1]]\n",
    "                    elif i2==y_spacings-1 and i1<x_spacings-1:\n",
    "                        min_edge=[each_spacings[0][i1],each_spacings[1][i2]]\n",
    "                        max_edge=[each_spacings[0][i1+1],max_y]\n",
    "                    elif i1==x_spacings-1 and i2==y_spacings-1:\n",
    "                        min_edge=[each_spacings[0][i1],each_spacings[1][i2]]\n",
    "                        max_edge=[max_x,max_y]\n",
    "                    else:\n",
    "                        min_edge=[each_spacings[0][i1],each_spacings[1][i2]]\n",
    "                        max_edge=[each_spacings[0][i1+1],each_spacings[1][i2+1]]\n",
    "                    all_spacings.append([min_edge,max_edge])\n",
    "                        \n",
    "                    #Get filter intervals (Numbering starts from zero)\n",
    "                    x_filter_intervals=each_intervals_in_spacings[0][i1]\n",
    "                    y_filter_intervals=each_intervals_in_spacings[1][i2]\n",
    "                    intervals=list(itertools.product(x_filter_intervals,list(np.array(y_filter_intervals)*x_intervals)))\n",
    "                    intervals=sorted([interval[0]+interval[1] for interval in intervals])\n",
    "                    all_intervals_in_spacings.append(intervals)\n",
    "        else: \n",
    "            assert self.dim<3, \"All_spacings not yet configured for 3D and higher filters\"\n",
    "        return all_spacings,all_intervals_in_spacings \n",
    "        \n",
    "        \n",
    "\n",
    "    class bins: #For 1-D\n",
    "        def __init__(self,intervals,gain,filt_min,filt_max):\n",
    "            assert filt_max>filt_min, \"Value for filter max should be greater than filter min\"\n",
    "            self.intervals=intervals\n",
    "            self.filt_min=filt_min\n",
    "            self.filt_max=filt_max\n",
    "            self.filt_length=float(self.filt_max)-self.filt_min\n",
    "            if gain>1: #Ayasdi's gain formula: %Overlap=1-1/gain for 1<gain<10\n",
    "                self.gain=1-(1./gain)\n",
    "            else:\n",
    "                self.gain=gain\n",
    "            self.bin_edges=[] \n",
    "            self.overlap_edges=[]\n",
    "    \n",
    "        def _get_intervals(self):\n",
    "            interval_length=self.filt_length/(self.intervals-(self.intervals-1)*self.gain)\n",
    "            step_size=interval_length*(1-self.gain)\n",
    "            #nogain_edges=[self.filt_min+(i*nogain_interval_length) for i in range(1,self.intervals)]\n",
    "            return interval_length,step_size\n",
    "    \n",
    "        def get_bin_edges(self):\n",
    "            interval_length,step_size=self._get_intervals()\n",
    "            bin_edges=[]\n",
    "            overlap_edges=[]\n",
    "            for i in range(self.intervals):\n",
    "                bin_edges.append(list(np.add([self.filt_min,self.filt_min+interval_length],i*step_size)))\n",
    "                if i>0:\n",
    "                    overlap_edges.append([bin_edges[i][0],bin_edges[i-1][1]])  \n",
    "            bin_edges[-1][-1]=self.filt_max\n",
    "           #Update self\n",
    "            self.bin_edges=bin_edges\n",
    "            self.overlap_edges=overlap_edges\n",
    "            return bin_edges,overlap_edges    \n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster, linkage    \n",
    "class Clustering:\n",
    "#Single-linkage clustering, Histogram method, for 2D filters    \n",
    "    def __init__(self, point_values_per_level):\n",
    "        self.point_values=point_values_per_level\n",
    "        self.point_idx_in_clusters=self._get_cluster_memberships()\n",
    "        \n",
    "    def _get_cluster_memberships(self):\n",
    "        #Agglomerative clustering by single-linkage\n",
    "        Z=linkage(self.point_values,'single') #Returns Z, linkage matrix as defined by scipy\n",
    "        #Get first empty gap in histogram of edge distances (default=10 bins)\n",
    "        edge_dist=[link[2] for link in Z]\n",
    "        histo_freq=list(np.histogram(edge_dist)[0])\n",
    "        try:\n",
    "            first_gap=histo_freq.index(0) #Find first gap in histogram\n",
    "        except:\n",
    "            return [range(len(self.point_values))] #No gaps in histogram. All points belong to same cluster.\n",
    "        \n",
    "        cluster_memberships=list(fcluster(Z,first_gap,criterion='distance'))\n",
    "        point_idx_in_clusters=[]\n",
    "        for cluster in np.unique(cluster_memberships):\n",
    "            point_idx_in_clusters.append(np.where([cluster_memberships[i]==cluster for i in range(len(cluster_memberships))])[0].tolist())\n",
    "        return point_idx_in_clusters\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Queue\n",
    "import random\n",
    "import time\n",
    "\n",
    "class THD:\n",
    "    def __init__(self,load_file_name=None,\n",
    "                 params_dict=None):\n",
    "                 \n",
    "#                 training_data,res_increase, \n",
    "#                 filter_fn,cov,cut,segmenter,metric='euclidean',training_labels=None,supervised_filter=False):\n",
    "        '''\n",
    "        Either enter load_file_name to load THD file (untested), or enter mapper parameters in params_dict\n",
    "        params_dict <dict>: training_data\n",
    "                            res_increase\n",
    "                            res_increment\n",
    "                            filter_fn\n",
    "                            cov\n",
    "                            cut\n",
    "                            segmenter\n",
    "                            metric: 'euclidean' <default>\n",
    "                            training_labels: None <default> Required if supervised filter_fn is used.\n",
    "                            supervised_filter: False <default> Set as True if supervised filter_fn is used.\n",
    "                            points_threshold: None <default> parameter for res_increase if it is a getres_2D.connectivity_check\n",
    "        Callable functions: get_model, draw_model, draw_mapper\n",
    "        '''\n",
    "            \n",
    "        if type(params_dict)!=type(None):\n",
    "            self.training_data=np.array(params_dict['training_data'])\n",
    "            self.res_increase=params_dict['res_increase']\n",
    "            self.res_increment=params_dict['res_increment'] #Alternative resolution increment if res_increase is a function        \n",
    "            self.sklearn_filter_fn=params_dict['sklearn_filter_fn'] #either unsupervised (takes data parameter) or supervised (takes data, labels parameters)\n",
    "            self.gain=params_dict['gain']\n",
    "            self.interval=params_dict['interval']\n",
    "            \n",
    "            self.segmenter=params_dict['segmenter'] #This should be an instantiated segmenter class with associated function \"get_segments\"\n",
    "\n",
    "            self.metric=params_dict['metric']\n",
    "            self.training_labels=params_dict['training_labels']\n",
    "            self.supervised_filter=params_dict['supervised_filter'] #Supervised filters=True: terminate THD when connected component has points of 1 unique label\n",
    "            self.points_threshold=params_dict['points_threshold']\n",
    "\n",
    "\n",
    "        self.model=[] #Hierarchy list of dictionaries with keywords /\n",
    "                #{'idx','parent','children','mapper_result','connected_components'}\n",
    "        self.next=0 #Next idx for model\n",
    "        \n",
    "        if type(load_file_name)!=type(None):\n",
    "            self.load_model(load_file_name)\n",
    "            self.training_data=self.training_data[0] #Strange problem where np.arrays are saved as (np.array)\n",
    "            self.training_labels=self.training_labels[0] #Strange problem where np.arrays are saved as (np.array)\n",
    "            \n",
    "        self.queue=Queue.Queue()        \n",
    "        \n",
    "        if type(self.training_labels) != None:\n",
    "            self.label_names, self.label_colors=self._get_label_colors()  \n",
    "                 \n",
    "            \n",
    "    def _get_filt(self,points_list,get_pcd=False):\n",
    "        data=self.training_data[points_list,:]  \n",
    "        pcd=pdist(data,self.metric)\n",
    "        try:\n",
    "            filt=self.filter_fn(data)\n",
    "        except: #Supervised filters require labels\n",
    "            labels=self.training_labels[points_list]\n",
    "            filt=self.filter_fn(data,labels)\n",
    "        while len(filt)<len(points_list): #some filters take time! \n",
    "            time.sleep(0.1) \n",
    "        if get_pcd==True:\n",
    "            return filt,pcd\n",
    "        else:\n",
    "            return filt\n",
    "            \n",
    "    def get_mapper_result(self,points_list,interval):\n",
    "        points=self.training_data[points_list]\n",
    "        new_mapper=Mapper(points,self.sklearn_filter_fn,self.gain,interval,training_labels=self.training_labels)\n",
    "        mapper_result=new_mapper.get_mapper()\n",
    "        \n",
    "        return mapper_result   \n",
    "    \n",
    "    def _assign_res(self,interval, next_res):\n",
    "        return next_res\n",
    "    \n",
    "    def _increase_res(self,interval,res_increment):\n",
    "        return interval+res_increment\n",
    "                           \n",
    "                        \n",
    "    def get_model(self,points_list=None): \n",
    "        if points_list==None:\n",
    "            points_list=np.array(range(int(np.shape(self.training_data)[0])))\n",
    "        if not type(self.res_increase)== int:\n",
    "            filt=self._get_filt(points_list)\n",
    "            gain=cov.fract_overlap[0]\n",
    "            connectivity_check=self.res_increase(filt=filt,gain=gain)\n",
    "            next_res=connectivity_check.get_n_gap(n=3,points_threshold=self.points_threshold)\n",
    "            assert type(next_res)!=type(None), \"First resolution not found\"\n",
    "            cov=self._assign_res(cov,next_res)\n",
    "        #Queue Mappers  \n",
    "        assert self.queue.empty()\n",
    "        assert self.next==0\n",
    "        item={'idx':self.next,'points_list':points_list,'interval':self.interval}\n",
    "        self.queue.put(item)\n",
    "        self.model.append({'idx':self.next,'parent':None,'points_list':np.array(points_list),'interval':self.interval})\n",
    "        self.next=self.next+1\n",
    "\n",
    "        #Main loop\n",
    "        while self.queue.empty()==False:\n",
    "            item=self.queue.get()\n",
    "            parent_interval=item['interval']\n",
    "            parent_points_list=item['points_list']\n",
    "            mapper_result=self.get_mapper_result(parent_points_list,parent_interval)\n",
    "            print \"Resolution used: %s\"%(parent_interval)\n",
    "            segments=self.segmenter.get_segments(mapper_result)\n",
    "            #Save mapper result to model\n",
    "            self.model[item['idx']]['mapper_result']=mapper_result\n",
    "            self.model[item['idx']]['segments']=segments\n",
    "            #Get number of children, queue them, and update hierarchy list \n",
    "            if len(segments)>0:\n",
    "                interval=parent_interval #copy parent interval\n",
    "                parent_idx=item['idx']\n",
    "                self.model[parent_idx]['children']=range(self.next,self.next+len(segments)) #update parent hierarchy entry\n",
    "                for segment in segments:\n",
    "                    points_list=parent_points_list[segment['points_list']] #Convert to point idx of global model\n",
    "                    if len(segments)==1:\n",
    "                        if type(self.res_increase)==int:\n",
    "                            new_interval=self._increase_res(interval,self.res_increase)  #Res increase if only 1 segment\n",
    "                        else: #If res_increase is a function...\n",
    "                            new_interval=self._increase_res(interval,self.res_increment) ##TBD\n",
    "                    elif len(segments)>1:\n",
    "                        if type(self.res_increase)==int:\n",
    "                            new_interval=parent_interval #Use parent_interval\n",
    "                        if not type(self.res_increase)==int: \n",
    "                            filt=self._get_filt(points_list)\n",
    "                            gain=self.gain\n",
    "                            test=self.res_increase(filt=filt,gain=gain) ##TBD\n",
    "                            next_res=test.get_n_gap(n=3,points_threshold=self.points_threshold)\n",
    "                            if next_res==None:\n",
    "                                new_interval=parent_interval #Use parent cover\n",
    "                            else:\n",
    "                                new_interval=self._assign_res(interval,next_res)                   \n",
    "\n",
    "                    item={'idx':self.next, 'points_list':points_list, 'interval':new_interval}\n",
    "                    self.model.append({'idx':self.next,'parent':parent_idx,'points_list':np.array(points_list),'interval':new_interval}) #update child hierarchy entry                \n",
    "                    if self.supervised_filter==True:\n",
    "                        if len(np.unique(self.training_labels[item['points_list']]))>1.1:\n",
    "                            self.queue.put(item) #Queue for mapper only if there is more than 1 unique label\n",
    "                    else:\n",
    "                        self.queue.put(item)\n",
    "                    self.next=self.next+1\n",
    "        return self.model\n",
    "    \n",
    "    #Draw mapper from mapper_result\n",
    "    def draw_model(self,label_method=None): #To do: Add labels\n",
    "        h=gv.Digraph(engine='dot')\n",
    "        h.graph_attr['size']=\"10,10\" #Hardcoded graph size\n",
    "        h.graph_attr['splines']='ortho'\n",
    "        h.format='svg'\n",
    "        for idx, mod in enumerate(self.model):\n",
    "            node_name=str(idx)+\": \"+ str(len(mod['points_list']))+\" points\"\n",
    "            if label_method=='mode':\n",
    "                node_label=mode(self.training_labels[mod['points_list']])[0][0]\n",
    "                node_color=str(self.label_colors[np.where(self.label_names==node_label)[0][0]])\n",
    "            elif label_method=='fraction':\n",
    "                label_count=[]\n",
    "                for node_label in self.label_names:\n",
    "                    count=np.count_nonzero(self.training_labels[mod['points_list']]==node_label)\n",
    "                    label_count.append(count)\n",
    "                label_count=np.divide(label_count,float(sum(label_count)))\n",
    "                node_color=str()\n",
    "                for i, freq in enumerate(label_count):\n",
    "                    if freq >0:\n",
    "                        node_color=node_color+self.label_colors[i]+';'+str(freq)+':'\n",
    "            if label_method != None:\n",
    "                h.node(str(idx),node_name+\"\\nInterval: \"+str(mod['interval']),style='wedged',fillcolor=node_color)\n",
    "            else: \n",
    "                h.node(str(idx),node_name+\"\\nInterval: \"+str(mod['interval']))\n",
    "                \n",
    "            try:\n",
    "                for child in mod['children']:\n",
    "                    h.edge(str(idx),str(child))\n",
    "            except:\n",
    "                pass\n",
    "        return h    \n",
    "    \n",
    "    def draw_mapper(self,model_entry=None,mapper_result=None,points_list=None,label_method='mode'):\n",
    "        '''\n",
    "        Either define model_entry or define mapper_result, points_list\n",
    "        \n",
    "        model_entry: <dict> contains keys mapper_result and points_list\n",
    "        mapper_result: <dict> can be found under key 'mapper_result' for each output in model\n",
    "        points_list: <list> Idx of points from the original training data. Need it to get the labels\n",
    "        label_method: <str> 'mode', 'fraction'\n",
    "                        In 'mode', node is colored by the most popular label\n",
    "                        In 'fraction', node is colored by proportion of labels \n",
    "        '''\n",
    "        if model_entry:\n",
    "            mapper_result=model_entry['mapper_result']\n",
    "            points_list=model_entry['points_list']\n",
    "        elif mapper_result ==None or points_list ==None:\n",
    "            raise ValueError('Parameters mapper_result and points_list not defined')\n",
    "        \n",
    "        h=gv.Digraph(name='mapper_graph',engine='neato',format='png')\n",
    "        h.graph_attr['size']=\"10,10\" #Hardcoded graph size\n",
    "        #.graph_attr['pack']='false'\n",
    "        h.edge_attr['dir']=\"none\"\n",
    "        nodes=mapper_result['nodes']\n",
    "        edges=mapper_result['edges']\n",
    "        for node in nodes:\n",
    "            node_size=float(np.log(len(node['points'])/200+1)) #Arbitrary hard-coded size constraints\n",
    "            if label_method=='mode':\n",
    "                node_label=mode(self.training_labels[points_list[node['points']]])[0][0]\n",
    "                node_color=str(self.label_colors[np.where(self.label_names==node_label)[0][0]])\n",
    "            elif label_method=='fraction':\n",
    "                label_count=[]\n",
    "                for node_label in self.label_names:\n",
    "                    count=np.count_nonzero(self.training_labels[points_list[node['points']]]==node_label)\n",
    "                    label_count.append(count)\n",
    "                label_count=np.divide(label_count,float(sum(label_count)))\n",
    "                node_color=str()\n",
    "                for idx, freq in enumerate(label_count):\n",
    "                    if freq >0:\n",
    "                        node_color=node_color+self.label_colors[idx]+';'+str(freq)+':'\n",
    "            h.node(str(node['idx']),shape='circle',label=str(len(node['points'])),height=str(node_size),style='wedged',fillcolor=node_color)\n",
    "        for edge in edges:\n",
    "            h.edge(str(edge[0]),str(edge[1]),penwidth=str(np.log(edges[edge]+1)))\n",
    "        \n",
    "        #Render mapper graph, legend and paste onto combined mapper_image\n",
    "        #mapper_graph=Image.open(h.render())\n",
    "        mapper_image=h\n",
    "        legend=self._get_legend_as_graph()\n",
    "        #mapper_image_size=(mapper_graph.size[0]+legend.size[0],mapper_graph.size[1])\n",
    "        #mapper_image=Image.new('RGB',mapper_image_size,color='white')\n",
    "        #mapper_image.paste(mapper_graph,(0,0))\n",
    "        #legend_pos=(mapper_graph.size[0],mapper_graph.size[1]-legend.size[1])\n",
    "        #mapper_image.paste(legend,legend_pos)\n",
    "        return mapper_image,legend\n",
    "    \n",
    "    def _get_label_colors(self):\n",
    "        label_names=np.unique(self.training_labels)\n",
    "        golden_ratio_conjugate=0.618033988749895\n",
    "        hue=0\n",
    "        label_colors=[]\n",
    "        for i in range(len(label_names)):\n",
    "            color=str(hue)+\" 0.5\"+ \" 0.95\"\n",
    "            label_colors.append(color)\n",
    "            hue=hue+golden_ratio_conjugate\n",
    "            hue=hue%1\n",
    "        return label_names,label_colors\n",
    "    \n",
    "    def _get_legend_as_graph(self):\n",
    "        h=gv.Digraph(engine='dot',format='png')\n",
    "        h.graph_attr['size']=str(2) #Hardcoded legend size\n",
    "        h.graph_attr['rankdir']='LR'\n",
    "        h.graph_attr['mindist']='0.0'\n",
    "        h.graph_attr['ranksep']='0.0'\n",
    "        h.graph_attr['nodesep']='0.0'\n",
    "        h.node_attr['shape']='box'\n",
    "        h.node_attr['margin']=\"0,0\"\n",
    "        h.node_attr['width']=\"1\"\n",
    "        h.node_attr['height']=\"0.5\"\n",
    "        h.edge_attr['style']='invis'\n",
    "        #h.edge_attr['minlen']='0'\n",
    "        for idx, label in enumerate(self.label_names):\n",
    "            color=str(self.label_colors[idx])\n",
    "            h.node(color[:3],style='filled',fillcolor=color,fontcolor=color)\n",
    "            h.edge('label '+str(label),color[:3])\n",
    "        return h\n",
    "\n",
    "    \n",
    "    def save_model(self,base_name):\n",
    "        assert self.queue.empty(), \"Queue is not empty, please wait for get_model() to be completed. \"\n",
    "        import pickle\n",
    "        struct_time=time.strftime(\"%Y-%m-%d %H:%M:%S\",time.gmtime())  \n",
    "        number=random.randint(0,99)\n",
    "        file_name=base_name+\"_\"+str(struct_time+\"_\"+str(number))\n",
    "        self.file_name=file_name\n",
    "        params_dict={\n",
    "            'training_data':self.training_data,\n",
    "            'file_name':self.file_name,\n",
    "            'res_increase':self.res_increase,\n",
    "            'res_increment':self.res_increment,\n",
    "            'sklearn_filter_fn':self.sklearn_filter_fn,\n",
    "            'gain':self.gain,\n",
    "            'interval':self.interval,\n",
    "            'segmenter':self.segmenter,\n",
    "            'metric':self.metric,\n",
    "            'training_labels':self.training_labels,\n",
    "            'supervised_filter':self.supervised_filter,\n",
    "            'points_threshold':self.points_threshold,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            params_dict['model']=self.model\n",
    "            params_dict['next']=self.next\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        with open(file_name,'w') as fp:\n",
    "            pickle.dump(params_dict, fp)\n",
    "            print \"Model successfully saved as %s.\" %(self.file_name)\n",
    "        return True\n",
    "        \n",
    "    def load_model(self,file_name):\n",
    "        import pickle\n",
    "        with open(file_name,'r') as fp:\n",
    "            params_dict=pickle.load(fp)        \n",
    "            \n",
    "        self.training_data=params_dict['training_data'], \n",
    "        self.file_name=params_dict['file_name'],\n",
    "        self.res_increase=params_dict['res_increase'],\n",
    "        self.res_increment=params_dict['res_increment'],\n",
    "        self.sklearn_filter_fn=params_dict['sklearn_filter_fn'],\n",
    "        self.gain=params_dict['gain'],\n",
    "        self.interval=params_dict['interval'],\n",
    "        self.segmenter=params_dict['segmenter'],\n",
    "        self.metric=params_dict['metric'],\n",
    "        self.training_labels=params_dict['training_labels'], \n",
    "        self.supervised_filter=params_dict['supervised_filter'],\n",
    "        self.points_threshold=params_dict['points_threshold']\n",
    "\n",
    "        try:\n",
    "            self.model=params_dict['model']\n",
    "            self.next=params_dict['next']\n",
    "            print \"Model successfully loaded.\"\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class topo_predict:\n",
    "    def __init__(self, mapper_result, training_data, training_labels, testing_data, k, prediction_method='mode', metric='euclidean'):\n",
    "\n",
    "        self.mapper_result=mapper_result\n",
    "        self.nodes=mapper_result['nodes']\n",
    "        self.training_data=training_data\n",
    "        self.testing_data=testing_data\n",
    "        self.training_labels=training_labels\n",
    "        self.k=k\n",
    "        self.prediction_method=prediction_method\n",
    "        self.metric=metric\n",
    "        self.predictions=[]\n",
    "        self.pt_membership=self._get_pt_membership()\n",
    "        \n",
    "        self.predict_all_testing()\n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        Represent the predictions of topo_predict by a string.\n",
    "        '''\n",
    "        return repr({\n",
    "            'mapper_result': self.mapper_result,\n",
    "            'testing_data': self.testing_data,\n",
    "            'testing_labels': self.testing_labels,\n",
    "            'k': self.k,\n",
    "            'prediction_method': self.prediction_method,\n",
    "            'metric': self.metric\n",
    "        })\n",
    "    def _get_pt_membership(self):\n",
    "        #Get the nodes each point belongs to\n",
    "        pt_membership=[[] for i in range(len(self.training_data))]\n",
    "        for node_idx in self.nodes.keys():\n",
    "            for pt_idx in self.nodes[node_idx]:\n",
    "                pt_membership[pt_idx].append(node_idx)\n",
    "        self.pt_membership=pt_membership\n",
    "        return pt_membership\n",
    "    def predict(self,testing_point): #Predict one testing point at a time\n",
    "        #Find nearest training point to testing point\n",
    "        dist=cdist(self.training_data,[testing_point],metric=self.metric)\n",
    "        dist=np.reshape(dist,(len(dist))) #Convert to 1-D array\n",
    "        closest_idx=dist.argsort()[0]\n",
    "        #Get nodes that nearest training point belong to\n",
    "        closest_nodes=self.pt_membership[closest_idx]\n",
    "        #Find k-NN in closest_nodes\n",
    "        points=[]\n",
    "        for node_idx in closest_nodes:\n",
    "            points.extend(self.nodes[node_idx])\n",
    "        points=sorted(np.unique(points))\n",
    "        k_points=list(dist[points].argsort()[:self.k])\n",
    "        neighbors_idx=np.array(points)[k_points]\n",
    "        if self.prediction_method=='mode':\n",
    "            from scipy.stats import mode\n",
    "            neighbors_labels=self.training_labels[neighbors_idx]\n",
    "            [[predicted_label],[freq]]=mode(neighbors_labels)\n",
    "            freq=float(freq)/self.k\n",
    "        prediction={'predicted_label':predicted_label,'closest_idx':closest_idx,'neighbors_idx':neighbors_idx,\n",
    "                   'neighbors_labels':neighbors_labels}\n",
    "        return prediction\n",
    "    def predict_all_testing(self): #Predict all testing points\n",
    "        assert self.predictions==[] #Raise error message 'predictions have already been made'\n",
    "        for test in self.testing_data:\n",
    "            prediction=self.predict(test)\n",
    "            self.predictions.append(prediction)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "class THD_predict(THD):\n",
    "    def __init__(self, load_predict_file=None,\n",
    "                 load_file_name=None,\n",
    "                 testing_data=None,\n",
    "                 testing_labels=None,\n",
    "                 neighbors_count=1,\n",
    "                 interpolation_source='containing-nodes',\n",
    "                 num_threads=8\n",
    "                ):\n",
    "        '''\n",
    "        Either call load_predict_file to load a THD_predict save file, or \n",
    "                call load_file_name to load THD_Ayasdi save file, with accompanying parameters.\n",
    "        '''        \n",
    "        if load_predict_file!=None:\n",
    "            self.load_predict_model(load_predict_file)\n",
    "        else:\n",
    "            THD.__init__(self,load_file_name=load_file_name)\n",
    "            \n",
    "            self.testing_data=testing_data\n",
    "            self.testing_labels=testing_labels\n",
    "            self.neighbors_count=neighbors_count\n",
    "            self.interpolation_source=interpolation_source\n",
    "            self.num_threads=num_threads\n",
    "        try:\n",
    "            self.activations\n",
    "        except:\n",
    "            self.activations=self._activations() #Dict of (test_idx: List of networks in THD hierarchy that the closest neighbor belongs) key:value pair\n",
    "        \n",
    "        self.queue=Queue.Queue()\n",
    "        self.queue_complete=False\n",
    "        \n",
    "    def _activations(self):\n",
    "        activations={}\n",
    "        for test_idx in range(len(self.testing_data)):\n",
    "            activations[str(test_idx)]=[0]\n",
    "        return activations\n",
    "    \n",
    "    def get_THD_predictions(self):\n",
    "        #Step1of3: Get activations\n",
    "        print \"Getting activations based on nearest neighbor.\"\n",
    "        activations=self._get_activations()\n",
    "        \n",
    "        #Step2of3: Get leaf activations\n",
    "        print \"Finding deepest (leaf) activations for each test point.\"\n",
    "        leaf_activations=[]\n",
    "        for test_idx in range(len(self.testing_data)):\n",
    "            leaf_activation=[]\n",
    "            activation=self.activations[str(test_idx)]\n",
    "            for network_idx in activation:\n",
    "                try:\n",
    "                    children_idx=self.model[network_idx]['children']\n",
    "                    if len(children_idx)<1: #Case3: Network has no children - IS a leaf activation\n",
    "                        leaf_activation.append(network_idx)\n",
    "                    elif any(np.isin(children_idx,activation)): #Case1: Child network is in activation - NOT a leaf activation\n",
    "                        pass\n",
    "                    else:\n",
    "                        leaf_activation.append(network_idx) #Case2: No child network in activation - IS a leaf activation\n",
    "                except: #Case3: Network has no children - IS a leaf activation\n",
    "                    leaf_activation.append(network_idx)\n",
    "            leaf_activations.append(leaf_activation)\n",
    "        self.leaf_activations=leaf_activations\n",
    "        \n",
    "        #Step3of3: Get predictions using leaf activation networks\n",
    "        print \"Getting predictions based on leaf activations.\"\n",
    "        self._get_leaf_predictions()\n",
    "        \n",
    "        return self.predictions\n",
    "    \n",
    "    def _get_activations(self):\n",
    "        test_idx = range(len(self.testing_data))\n",
    "        #Queue activations\n",
    "        assert self.queue.empty(), \"Queue is not empty, please re-initialize THD_predict.\"\n",
    "        self.queue_complete=False\n",
    "        item={'idx':0,'test_idx':test_idx}\n",
    "        self.queue.put(item)\n",
    "        \n",
    "        #Main loop\n",
    "        threads=[]\n",
    "        for i in range(self.num_threads):\n",
    "            t=Thread(target=self._get_activations_worker, name=i)\n",
    "            t.daemon=True\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        self.queue.join()\n",
    "        self.queue_complete=True\n",
    "        return self.activations\n",
    "    \n",
    "    def _get_activations_worker(self):\n",
    "        while self.queue_complete==False:\n",
    "            try:\n",
    "                item=self.queue.get()\n",
    "                parent_idx=item['idx']\n",
    "                parent_test_idx=item['test_idx']\n",
    "                mapper_result=self.model[parent_idx]['mapper_result']\n",
    "                training_data=self.training_data[self.model[parent_idx]['points_list']]\n",
    "                training_labels=self.training_labels[self.model[parent_idx]['points_list']]\n",
    "                testing_data=self.testing_data[parent_test_idx]\n",
    "                #Get predictions\n",
    "                predictions=topo_predict(mapper_result, \n",
    "                                         training_data, \n",
    "                                         training_labels, \n",
    "                                         testing_data,\n",
    "                                         self.neighbors_count,\n",
    "                                         prediction_method='mode',\n",
    "                                         metric='euclidean'\n",
    "                                        ).predictions  \n",
    "                self.model[parent_idx]['topo_predict_result']=predictions\n",
    "                #Get children activations\n",
    "                try: \n",
    "                    self.model[parent_idx]['children']\n",
    "                    child_test_idx=self._get_next_test_group_and_activation(parent_idx,parent_test_idx,predictions)\n",
    "                    for child_network in child_test_idx:\n",
    "                        if len(child_network['test_idx'])>0:\n",
    "                            self.queue.put(child_network)\n",
    "                except: #Parent network has not children\n",
    "                    pass\n",
    "                print \"Finished getting activations for parent network %s.\" %(parent_idx)\n",
    "                print \"Items left in queue: %s\" %(self.queue.qsize())\n",
    "                self.queue.task_done()\n",
    "            except:\n",
    "                pass\n",
    "#                try:\n",
    "#                    self.queue.task_done()\n",
    "#                    print \"Having trouble with parent network %s. Re-submitting to queue.\" %(parent_idx)\n",
    "#                    self.queue.put(item)\n",
    "#                except:\n",
    "#                    print \"Unable to resubmit item to queue.\"\n",
    "                \n",
    "    def _get_next_test_group_and_activation(self,parent_idx,parent_test_idx, prediction):\n",
    "        activation=[[]*len(testing_data)]\n",
    "        children_idx=self.model[parent_idx]['children']\n",
    "        parent_training_point_idx=self.model[parent_idx]['points_list']\n",
    "        nn_array=np.array([parent_training_point_idx[pred['closest_idx']] for pred in prediction]) #Note that closest_idx is relative to parent's training_idx\n",
    "        child_test_idx=[]\n",
    "        for child_idx in children_idx:\n",
    "            #Array of binary values: if nearest-neighbor of test-row is found in child network, then True. Else, False.\n",
    "            match_array=np.isin(nn_array,self.model[child_idx]['points_list'])\n",
    "            activated_idx=np.where(match_array)[0]\n",
    "            activated_test_idx=list(np.array(parent_test_idx)[activated_idx]) #Convert activated_idx to test_idx\n",
    "            for test_idx in activated_test_idx:\n",
    "                self.activations[str(test_idx)].append(child_idx) #Save activation\n",
    "            child_test_idx.append({'idx':child_idx,'test_idx':activated_test_idx})\n",
    "        return child_test_idx\n",
    "        \n",
    "    def _get_leaf_predictions(self):\n",
    "        self.predictions=[[] for i in range(len(self.leaf_activations))]\n",
    "        assert self.queue.empty(), \"Queue is not empty, please re-initialize THD_predict.\"\n",
    "        self.queue_complete=False\n",
    "        self.queue=Queue.Queue()\n",
    "        for network_idx in range(len(self.model)):\n",
    "            self.queue.put(network_idx)\n",
    "        #Main loop\n",
    "        threads=[]\n",
    "        for i in range(self.num_threads):\n",
    "            t=Thread(target=self._get_leaf_prediction_worker,name=i)\n",
    "            t.daemon=True\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "            \n",
    "        self.queue.join()\n",
    "        self.queue_complete=True\n",
    "        return self.predictions\n",
    "    \n",
    "    def _get_leaf_prediction_worker(self):\n",
    "        while self.queue_complete==False:\n",
    "            print self.queue.qsize()\n",
    "            network_idx=self.queue.get()\n",
    "            test_idx=np.where([np.isin(network_idx,leaf_activation) for leaf_activation in self.leaf_activations])[0]\n",
    "            mapper_result=self.model[network_idx]['mapper_result']\n",
    "            training_data=self.training_data[self.model[network_idx]['points_list']]\n",
    "            training_labels=self.training_labels[self.model[network_idx]['points_list']]\n",
    "            testing_data=self.testing_data[test_idx]            \n",
    "            \n",
    "            #Get predictions\n",
    "            predictions=topo_predict(mapper_result, \n",
    "                                         training_data, \n",
    "                                         training_labels, \n",
    "                                         testing_data,\n",
    "                                         self.neighbors_count,\n",
    "                                         prediction_method='mode',\n",
    "                                         metric='euclidean'\n",
    "                                        ).predictions \n",
    "            self.model[network_idx]['leaf_prediction_result']=predictions\n",
    "            for idx,test_point_idx in enumerate(test_idx):\n",
    "                self.predictions[test_point_idx].extend([predictions[idx]['predicted_label']])\n",
    "            print \"Finished getting predictions for Network %s\" %(str(network_idx))\n",
    "            print \"Current queue size: %s\" %(self.queue.qsize())\n",
    "            self.queue.task_done()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ayasdi.core.api import Api\n",
    "#Log in to Ayasdi\n",
    "connection=Api(user_name,user_password)\n",
    "src=connection.get_source(name='XY_MNIST_ConvolutionalSet.csv')\n",
    "training_labels=np.array(src.export(column_indices=[2])['data'])[0]\n",
    "\n",
    "#Get first 5k MNIST258\n",
    "points_list=[]\n",
    "for idx,label in enumerate(training_labels):\n",
    "    if label%3==2:\n",
    "        points_list.append(int(idx))\n",
    "    if len(points_list)>4999:\n",
    "        break\n",
    "training_labels=training_labels[points_list]\n",
    "\n",
    "training_data=np.array(src.export(column_indices=range(4,1546),row_indices=points_list)['data'])\n",
    "training_data=np.transpose(training_data)\n",
    "\n",
    "#Get 258's in testing set\n",
    "testing_labels=np.array(src.export(column_indices=[3],row_indices=range(60000,70000))['data'])[0]\n",
    "testing_points_list=[]\n",
    "for idx,label in enumerate(testing_labels):\n",
    "    if label%3==2:\n",
    "        testing_points_list.append(int(idx))\n",
    "testing_labels=testing_labels[testing_points_list]\n",
    "testing_points_list=[testing_points_list[i]+60000 for i in range(len(testing_points_list))]\n",
    "testing_data=np.array(src.export(column_indices=range(4,1546),row_indices=testing_points_list)['data'])\n",
    "testing_data=np.transpose(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "gain=0.2\n",
    "interval=5\n",
    "\n",
    "new_mapper=Mapper(training_data,TSNE,gain,interval,training_labels=training_labels)\n",
    "mapper_result=new_mapper.get_mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_mapper.draw_mapper(mapper_result=mapper_result,points_list=new_mapper.points, label_method='fraction')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "gain =0.2\n",
    "interval=5\n",
    "from mapper_extensions.Segmenters.Connected_Component_Segmenter import Connected_Component_Segmenter\n",
    "connected_component_segmenter=Connected_Component_Segmenter(network_threshold=5)\n",
    "params_dict={\n",
    "    'training_data':training_data,\n",
    "    'res_increase':1,\n",
    "    'res_increment':None,\n",
    "    'sklearn_filter_fn':TSNE,\n",
    "    'gain':gain,\n",
    "    'interval':interval,\n",
    "    'segmenter':connected_component_segmenter,\n",
    "    'metric':'euclidean',\n",
    "    'training_labels':training_labels,\n",
    "    'supervised_filter':False,\n",
    "    'points_threshold':5\n",
    "}\n",
    "\n",
    "new_THD2=THD(params_dict=params_dict)\n",
    "new_THD2.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_THD2.draw_model(label_method='fraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing time taken between python mapper and Xiu's mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "gain=0.1\n",
    "interval=5\n",
    "new_mapper=Mapper(training_data,TSNE,gain,interval,training_labels=training_labels)\n",
    "mapper_result=new_mapper.get_mapper()\n",
    "end=time.time()\n",
    "print(\"Time taken for Xiu's mapper: %.2f s.\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapper.cover import cube_cover_primitive\n",
    "from mapper.cutoff import histogram\n",
    "from mapper import mapper\n",
    "from scipy.spatial.distance import pdist\n",
    "start=time.time()\n",
    "cov=cube_cover_primitive(intervals=5, overlap=10)\n",
    "cut=histogram(10) #Overlap default=50\n",
    "pcd=pdist(training_data,'euclidean')\n",
    "filt=TSNE().fit_transform(training_data).astype('float')\n",
    "mapper_result2=mapper(pcd, filt, cov, cut)\n",
    "end=time.time()\n",
    "print(\"Time taken for python mapper: %.2f s.\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
